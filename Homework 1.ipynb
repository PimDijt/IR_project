{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier is ons fantastische huiswerk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "possible_rankings = []\n",
    "\n",
    "possible_evals = 3\n",
    "\n",
    "for i in range(possible_evals):\n",
    "    for j in range(possible_evals):\n",
    "        for k in range(possible_evals):\n",
    "            for l in range(possible_evals):\n",
    "                for m in range(possible_evals):\n",
    "                    new_list = [i, j, k, l, m]\n",
    "                    possible_rankings.append(new_list)\n",
    "\n",
    "p_rankings = possible_rankings\n",
    "e_rankings = possible_rankings\n",
    "\n",
    "possible_combinations = []\n",
    "for comb in itertools.combinations(possible_rankings, 2):\n",
    "    possible_combinations.append(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection – pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(ranking, rank):\n",
    "    relevant = 0\n",
    "    for i in range(rank):\n",
    "        if ranking[i] > 0:\n",
    "            relevant += 1\n",
    "    return relevant /rank\n",
    "\n",
    "def NDCG(ranking, rank):\n",
    "    total_dcg = 0\n",
    "    total_ideal_dcg = 0\n",
    "\n",
    "    for i in range(rank):\n",
    "        total_dcg += ((2**ranking[i])-1)/math.log2(1+i+1)\n",
    "        ideal_ranking = sorted(ranking, reverse=True)\n",
    "        total_ideal_dcg += ((2**ideal_ranking[i])-1)/math.log2(1+i+1)\n",
    "    \n",
    "    try:\n",
    "        result = total_dcg/total_ideal_dcg\n",
    "    except:\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "def ERR(ranking, rank):\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    for i in range(rank):\n",
    "        relevant = relevance_map(ranking[i])\n",
    "        ERR += p * relevant / (i+1)\n",
    "        p = p * (1-relevant)\n",
    "    return ERR\n",
    "\n",
    "def relevance_map(relevant):\n",
    "    max_relevant = 2\n",
    "    return (2**relevant - 1)/2**max_relevant\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.5437713091520254\n",
      "0.12083333333333332\n",
      "0.4 0.5012658353418872 0.225\n",
      "0.4 0.5437713091520254 0.12083333333333332\n"
     ]
    }
   ],
   "source": [
    "p_precision = precision(p_rankings[10], 5)\n",
    "print(p_precision)\n",
    "\n",
    "p_ndcg = NDCG(p_rankings[10], 5)\n",
    "print(p_ndcg)\n",
    "\n",
    "p_err = ERR(p_rankings[10], 5)\n",
    "print(p_err)\n",
    "\n",
    "test = ([0, 0, 0, 2, 2], [0, 0, 1, 0, 1])\n",
    "\n",
    "p_precision = precision(test[0], 5)\n",
    "e_precision = precision(test[1], 5)\n",
    "\n",
    "p_ndcg = NDCG(test[0], 5)\n",
    "e_ndcg = NDCG(test[1], 5)\n",
    "\n",
    "p_err = ERR(test[0], 5)\n",
    "e_err = ERR(test[1], 5)\n",
    "\n",
    "print(p_precision, p_ndcg, p_err)\n",
    "print(e_precision, e_ndcg, e_err)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate the 𝛥measure (0 points)\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: 𝛥measure = measureE-measureP. Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26576\n"
     ]
    }
   ],
   "source": [
    "good_combinations = []\n",
    "\n",
    "for comb in possible_combinations:\n",
    "    p_precision = precision(comb[0], 5)\n",
    "    e_precision = precision(comb[1], 5)\n",
    "    \n",
    "    p_ndcg = NDCG(comb[0], 5)\n",
    "    e_ndcg = NDCG(comb[1], 5)\n",
    "      \n",
    "    p_err = ERR(comb[0], 5)\n",
    "    e_err = ERR(comb[1], 5)\n",
    "    \n",
    "    p_total = p_precision + p_ndcg + p_err\n",
    "    e_total = e_precision + e_ndcg + e_err\n",
    "    \n",
    "    if(e_total > p_total):\n",
    "        good_combinations.append(comb)\n",
    "        \n",
    "print(len(good_combinations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def balanced_interleave(P, E):\n",
    "    Kp = 0\n",
    "    Ke = 0\n",
    "    result = []\n",
    "    p_firsrt = False\n",
    "    coin = random.uniform(0,1)\n",
    "    if coin > .5:\n",
    "        p_first = True\n",
    "    \n",
    "    while Kp < len(P) or Ke < len(E):\n",
    "        if Kp < Ke or (Kp == Ke and p_first):\n",
    "            result.append({\"rel\": P[Kp], \"source\": \"P\"})\n",
    "            Kp += 1\n",
    "        else:\n",
    "            result.append({\"rel\": E[Ke], \"source\": \"E\"})\n",
    "    return result\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], [])\n",
      "([], [0, 0, 0])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-f5a2c7ecc7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcomb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgood_combinations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mteam_draft_interleave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mrankings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-4ee9daf0aba7>\u001b[0m in \u001b[0;36mteam_draft_interleave\u001b[0;34m(P, E)\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mmax_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mrel_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel_pos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"relevance\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_rel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"source\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mteamP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_rel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "for comb in good_combinations:\n",
    "    print(comb)\n",
    "    result = balanceinterleave(comb[0], comb[1])\n",
    "    rankings.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Implement User Clicks Simulation (15 points)\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "We have considered a number of click models including:<br />\n",
    "- Random Click Model (RCM)<br />\n",
    "- Position-Based Model (PBM)<br />\n",
    "- Simple Dependent Click Model (SDCM)<br />\n",
    "- Simple Dynamic Bayesian Network (SDBN)<br />\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.<br />\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RCM:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.p = 0\n",
    "    \n",
    "    def train(self, data):\n",
    "        #p = #clicks / #documents\n",
    "        documents = 0\n",
    "        clicks = 0\n",
    "        for row in data:\n",
    "            line = row.split()\n",
    "            if line[2] == \"Q\":\n",
    "                documents += len(line)-5\n",
    "            else:\n",
    "                clicks += 1\n",
    "        self.p = clicks/documents\n",
    "                \n",
    "\n",
    "class PBM:\n",
    "    def __init__(self):\n",
    "        self.p = dict()\n",
    "        \n",
    "    def train(self, data):\n",
    "        documents = Counter()\n",
    "        clicks = Counter()\n",
    "        \n",
    "        for row in data:\n",
    "            line = row.split()\n",
    "            if line[2] == \"Q\":\n",
    "                cur_urls = dict()\n",
    "                for i in range(0, len(line)-5):\n",
    "                    cur_urls[i] = line[i+5]\n",
    "                    documents[i] += 1\n",
    "            else:\n",
    "                url_click = line[3]\n",
    "                url_position = 0\n",
    "                for pos, url in cur_urls.items():\n",
    "                    if url_click == url:\n",
    "                        url_poistion = pos\n",
    "                        break\n",
    "                clicks[pos] += 1\n",
    "        for pos, document_count in documents.items():\n",
    "            self.p[pos] = clicks[pos]/document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_click_model = RCM()\n",
    "position_click_model = PBM()\n",
    "\n",
    "with open('YandexRelPredChallenge.txt') as f:\n",
    "    random_click_model.train(f)\n",
    "\n",
    "with open('YandexRelPredChallenge.txt') as f:\n",
    "    position_click_model.train(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "\n",
    "(Note 7: Some of the models above include an attractiveness parameter 𝑎uq. Use the relevance label to assign this parameter by setting 𝑎uq for a document u in the ranked list accordingly. (See Click Models for Web Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_random_model(rankings, N):\n",
    "    total = N * len(rankings)\n",
    "    e_wins = 0\n",
    "    for i in range(N):\n",
    "        for url in rankings:\n",
    "            coin = random.uniform(0,1)\n",
    "            if coin < random_click_model and url.source == \"e\":\n",
    "                e_wins += 1\n",
    "    return e_wins / total\n",
    "\n",
    "def simulate_position_model(rankings, N):\n",
    "    total = N * len(rankings)\n",
    "    e_wins = 0\n",
    "    for i in range(N):\n",
    "        for j in range(len(rankings)):\n",
    "            coin = random.uniform(0,1)\n",
    "            if coin < position_click_model.p[j]*rankings[j].relevance and rankings[j].source == \"e\":\n",
    "                e_wins += 1\n",
    "    return e_wins / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Results and Analysis (30 points)\n",
    "Compare the results of the offline experiments (i.e. the values of the 𝛥measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "Use easy to read and comprehend visuals to demonstrate the results;\n",
    "Analyze the results on the basis of:\n",
    "- the evaluation measure used,\n",
    "- the interleaving method used,\n",
    "- the click model used.\n",
    "\n",
    "Report and ground your conclusions.\n",
    "\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
