{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Evaluation\n",
    "Deadline: Wednesday, January 17th, midnight (23:59)<br>\n",
    "Collaboration: This is a team-based assignment. Form teams of 3 people.<br>\n",
    "Submit: An IPython Notebook for both the theoretical and the experimental parts with the necessary (a) answers, (b) implementation, (c) explanations,  (d) comments, and (e) analysis. Code quality, informative comments, detailed explanations of what each block in the notebook does and more important convincing analysis of the results will be considered when grading.<br>\n",
    "Submit your work through Blackboard. All students in a team need to submit their work. <br>\n",
    "Filename: <student 1 id>–<student 2 id>–<student 3 id>–hw1.ipynb <br>\n",
    "The homework will cover the following three topics covered in Lecture 1, 2, and 3:<br>\n",
    "Evaluation measures;<br>\n",
    "Interleaving;<br>\n",
    "Click models.<br>\n",
    "The homework has a small theoretical part and a large experimental part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical part\n",
    "\n",
    "## Hypothesis Testing – The problem of multiple comparisons [5 points]\n",
    "\n",
    "Experimentation in AI often happens like this:<br>\n",
    "A. Modify/Build an algorithm<br>\n",
    "B. Compare the algorithm to a baseline by running a hypothesis test.<br>\n",
    "C. If not significant, go back to step A<br>\n",
    "D. If significant, start writing a paper.<br>\n",
    "\n",
    "Compute the probabilities below (with Type I error for each test = α):<br>\n",
    "P(mth experiment gives significant result | m experiments lacking power to reject H0)?\n",
    "\n",
    "If you mean that m-1 experiments have not given significance and the mth has, then: ((1-α)^(m-1))α<br>\n",
    "If you mean that m experiments have not given significance and the m+1th has, then: ((1-α)^(m))α\n",
    "\n",
    "P(at least one significant result | m experiments lacking power to reject H0)?\n",
    "\n",
    "(1-α)^m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and unfairness in Interleaving experiments [10 points]¶\n",
    "\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ⅔ of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n",
    "\n",
    "Let's assume that A is the better algorithm here.\n",
    "\n",
    "Algorithm A &nbsp; &nbsp;   Algorithm B<br>\n",
    "Link 2   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   Link 3<br>\n",
    "Link 3   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   Link 4<br>\n",
    "Link 4   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   Link 5<br>\n",
    "\n",
    "In lists with a distribution like this, the full search page list could be biased to Algorithm B; if B starts the second round, the resulting list would look like this:\n",
    "\n",
    "B: Link 3<br>\n",
    "A: Link 2<br>\n",
    "B: Link 4<br>\n",
    "B: Link 5<br>\n",
    "\n",
    "Therefore Algorithm B would win 3/4th of all random clicks, resulting in an bias pro algorithm B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental part\n",
    "Commercial search engines use both offline and online approach in evaluating a new search algorithm: they first use an offline test collection to compare the production algorithm (P) with the new experimental algorithm (E); if E statistically significantly outperforms P with respect to the evaluation measure of their interest, the two algorithms are then compared online through an interleaving experiment.<br>\n",
    "For the purpose of this homework we will assume that the evaluation measures of interest are:<br>\n",
    "1. Binary evaluation measures<br>\n",
    "a. Precision at rank k,<br>\n",
    "b. Recall at rank k,<br>\n",
    "c. Average Precision,<br>\n",
    "2. Multi-graded evaluation measures<br>\n",
    "a. Normalized Discounted Cumulative Gain at rank k (nDCG@k),<br>\n",
    "b. Expected Reciprocal Rank (ERR).<br>\n",
    "\n",
    "Further, for the purpose of this homework we will assume that the interleaving algorithms of interest are:\n",
    "1. Team-Draft Interleaving (Joachims. \"Evaluating retrieval performance using clickthrough data\". Text Mining 2003.)\n",
    "2. <strike>Probabilistic Interleaving (Hofmann, Whiteson, and de Rijke. \"A probabilistic method for inferring preferences from clicks.\" CIKM 2011.).</strike>\n",
    " \n",
    "In an interleaving experiment the ranked results of P and E (against a user query) are interleaved in a single ranked list which is presented to a user. The user then clicks on the results and the algorithm that receives most of the clicks wins the comparison. The experiment is repeated for a number of times (impressions) and the total wins for P and E are computed. \n",
    "\n",
    "A Sign/Binomial Test is then run to examine whether the difference in wins between the two algorithms is statistically significant (or due to chance). Alternatively one can calculate the proportion of times the E wins and test whether this proportion, p, is greater than p0=0.5. This is called an 1-sample 1-sided proportion test.\n",
    "\n",
    "One of the key questions however is <b>whether offline evaluation and online evaluation outcomes agree with each other</b>. In this homework you will determine the degree of agreement between offline evaluation measures and interleaving outcomes, by the means of simulations. A similar analysis using actual online data can be found at Chapelle et al. “Large-Scale Validation and Analysis of Interleaved Search Evaluation”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on lecture 1\n",
    "## Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "possible_rankings = []\n",
    "\n",
    "possible_evals = 3\n",
    "\n",
    "for i in range(possible_evals):\n",
    "    for j in range(possible_evals):\n",
    "        for k in range(possible_evals):\n",
    "            for l in range(possible_evals):\n",
    "                for m in range(possible_evals):\n",
    "                    new_list = [i, j, k, l, m]\n",
    "                    possible_rankings.append(new_list)\n",
    "\n",
    "p_rankings = possible_rankings\n",
    "e_rankings = possible_rankings\n",
    "\n",
    "possible_combinations = []\n",
    "for comb in itertools.combinations(possible_rankings, 2):\n",
    "    possible_combinations.append(comb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n",
    "(Note 2: Some of the aforementioned measures require the total number of relevant and highly relevant documents in the entire collection – pay extra attention on how to find this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision(ranking, rank):\n",
    "    relevant = 0\n",
    "    for i in range(rank):\n",
    "        if ranking[i] > 0:\n",
    "            relevant += 1\n",
    "    return relevant /rank\n",
    "\n",
    "def NDCG(ranking, rank):\n",
    "    total_dcg = 0\n",
    "    total_ideal_dcg = 0\n",
    "\n",
    "    for i in range(rank):\n",
    "        total_dcg += ((2**ranking[i])-1)/math.log2(1+i+1)\n",
    "        ideal_ranking = sorted(ranking, reverse=True)\n",
    "        total_ideal_dcg += ((2**ideal_ranking[i])-1)/math.log2(1+i+1)\n",
    "    \n",
    "    try:\n",
    "        result = total_dcg/total_ideal_dcg\n",
    "    except:\n",
    "        result = 0\n",
    "    return result\n",
    "\n",
    "def ERR(ranking, rank):\n",
    "    p = 1\n",
    "    ERR = 0\n",
    "    for i in range(rank):\n",
    "        R = satisfy_prob(ranking[i])\n",
    "        ERR += p * R / (i+1)\n",
    "        p *= (1-R)\n",
    "    return ERR\n",
    "\n",
    "def satisfy_prob(relevance):\n",
    "    max_relevant = 2\n",
    "    return (2**relevance - 1)/2**max_relevant  #so probability is always smaller than 1, max 3/4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "0.5437713091520254\n",
      "0.12083333333333332\n",
      "0.4 0.5012658353418872 0.225\n",
      "0.4 0.5437713091520254 0.12083333333333332\n"
     ]
    }
   ],
   "source": [
    "p_precision = precision(p_rankings[10], 5)\n",
    "print(p_precision)\n",
    "\n",
    "p_ndcg = NDCG(p_rankings[10], 5)\n",
    "print(p_ndcg)\n",
    "\n",
    "p_err = ERR(p_rankings[10], 5)\n",
    "print(p_err)\n",
    "\n",
    "test = ([0, 0, 0, 2, 2], [0, 0, 1, 0, 1])\n",
    "\n",
    "p_precision = precision(test[0], 5)\n",
    "e_precision = precision(test[1], 5)\n",
    "\n",
    "p_ndcg = NDCG(test[0], 5)\n",
    "e_ndcg = NDCG(test[1], 5)\n",
    "\n",
    "p_err = ERR(test[0], 5)\n",
    "e_err = ERR(test[1], 5)\n",
    "\n",
    "print(p_precision, p_ndcg, p_err)\n",
    "print(e_precision, e_ndcg, e_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate the 𝛥measure (0 points)\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: 𝛥measure = measureE-measureP. Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16286 out of 29403 combinations found where the precision for that combination is better for E than for P.\n",
      "There are 25931 out of 29403 combinations found where the NDCG score for that combination is better for E than for P.\n",
      "There are 27868 out of 29403 combinations found where the ERR score for that combination is better for E than for P.\n",
      "There are 26576 out of 29403 combinations found where the combined score over all three measures for that combination is better for E than for P.\n"
     ]
    }
   ],
   "source": [
    "good_precision = []\n",
    "good_NDCG = []\n",
    "good_ERR = []\n",
    "good_combinations = []\n",
    "\n",
    "for comb in possible_combinations:\n",
    "    \n",
    "    p_precision = precision(comb[0], 5)\n",
    "    e_precision = precision(comb[1], 5)\n",
    "    \n",
    "    if(e_precision > p_precision):\n",
    "        good_precision.append(e_precision)\n",
    "    \n",
    "    p_ndcg = NDCG(comb[0], 5)\n",
    "    e_ndcg = NDCG(comb[1], 5)\n",
    "    \n",
    "    if(e_ndcg > p_ndcg):\n",
    "        good_NDCG.append(e_ndcg)\n",
    "      \n",
    "    p_err = ERR(comb[0], 5)\n",
    "    e_err = ERR(comb[1], 5)\n",
    "    \n",
    "    if(e_err > p_err):\n",
    "        good_ERR.append(e_err)    \n",
    "    \n",
    "    p_total = p_precision + p_ndcg + p_err\n",
    "    e_total = e_precision + e_ndcg + e_err\n",
    "    \n",
    "    if(e_total > p_total):\n",
    "        good_combinations.append(comb)\n",
    "        \n",
    "print(\"There are %d out of %d combinations found where the precision for that combination is better for E than for P.\" % (len(good_precision), (len(possible_combinations))))\n",
    "print(\"There are %d out of %d combinations found where the NDCG score for that combination is better for E than for P.\" % (len(good_NDCG), (len(possible_combinations))))\n",
    "print(\"There are %d out of %d combinations found where the ERR score for that combination is better for E than for P.\" % (len(good_ERR), (len(possible_combinations))))\n",
    "print(\"There are %d out of %d combinations found where the combined score over all three measures for that combination is better for E than for P.\" % (len(good_combinations), (len(possible_combinations))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on lecture 2\n",
    "# Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, <strike>AND (2), Probabilistic Interleaving</strike>. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def balanced_interleave(P, E):\n",
    "    Kp = 0\n",
    "    Ke = 0\n",
    "    result = []\n",
    "    p_first = False\n",
    "    coin = random.uniform(0,1)\n",
    "    if coin > .5:\n",
    "        p_first = True\n",
    "    \n",
    "    while Kp < len(P) or Ke < len(E):\n",
    "        if Kp < Ke or (Kp == Ke and p_first):\n",
    "            result.append({\"rel\": P[Kp], \"source\": \"P\"})\n",
    "            Kp += 1\n",
    "        else:\n",
    "            result.append({\"rel\": E[Ke], \"source\": \"E\"})\n",
    "            Ke += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rankings = []\n",
    "for comb in good_combinations:\n",
    "    result = balanced_interleave(comb[0], comb[1])\n",
    "    rankings.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "based on lecture 3\n",
    "# Step 5: Implement User Clicks Simulation (15 points)\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "We have considered a number of click models including:<br />\n",
    "- Random Click Model (RCM)<br />\n",
    "- Position-Based Model (PBM)<br />\n",
    "- Simple Dependent Click Model (SDCM)<br />\n",
    "- Simple Dynamic Bayesian Network (SDBN)<br />\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.<br />\n",
    "\n",
    "Having implemented the two click models, estimate the model parameters using the Yandex Click Log \n",
    "\n",
    "(Note 6: Do not learn the attractiveness parameter 𝑎_uq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class RCM:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.p = 0\n",
    "    \n",
    "    def train(self, data):\n",
    "        #p = #clicks / #documents\n",
    "        documents = 0\n",
    "        clicks = 0\n",
    "        for row in data:\n",
    "            line = row.split()\n",
    "            if line[2] == \"Q\":\n",
    "                documents += len(line)-5\n",
    "            else:\n",
    "                clicks += 1\n",
    "        self.p = clicks/documents\n",
    "                \n",
    "\n",
    "class PBM:\n",
    "    def __init__(self):\n",
    "        self.p = dict()\n",
    "        \n",
    "    def train(self, data):\n",
    "        documents = Counter()\n",
    "        clicks = Counter()\n",
    "        \n",
    "        for row in data:\n",
    "            line = row.split()\n",
    "            if line[2] == \"Q\":\n",
    "                cur_urls = dict()\n",
    "                for i in range(0, len(line)-5):\n",
    "                    cur_urls[i] = line[i+5]\n",
    "                    documents[i] += 1\n",
    "            else:\n",
    "                url_click = line[3]\n",
    "                for pos, url in cur_urls.items():\n",
    "                    if url_click == url:\n",
    "                        break\n",
    "                clicks[pos] += 1\n",
    "        for pos, document_count in documents.items():\n",
    "            self.p[pos] = clicks[pos]/document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13445559411047547\n",
      "{0: 0.48844133921035354, 1: 0.20608646722310794, 2: 0.14632373628434775, 3: 0.11019412923192347, 4: 0.08529494513739098, 5: 0.07064147050548626, 6: 0.06203695020163181, 7: 0.055097064615961734, 8: 0.05104098283785051, 9: 0.06939885585670075}\n"
     ]
    }
   ],
   "source": [
    "random_click_model = RCM()\n",
    "position_click_model = PBM()\n",
    "\n",
    "with open('YandexRelPredChallenge.txt') as f:\n",
    "    random_click_model.train(f)\n",
    "\n",
    "with open('YandexRelPredChallenge.txt') as f:\n",
    "    position_click_model.train(f)\n",
    "    \n",
    "print(random_click_model.p)\n",
    "print(position_click_model.p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "\n",
    "(Note 7: Some of the models above include an attractiveness parameter 𝑎uq. Use the relevance label to assign this parameter by setting 𝑎uq for a document u in the ranked list accordingly. (See Click Models for Web Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulate_random_model(ranking, N):\n",
    "    e_wins = 0\n",
    "    for i in range(N):\n",
    "        for url in ranking:\n",
    "            coin = random.uniform(0,1)\n",
    "            if coin < random_click_model.p and url[\"source\"] == \"E\":\n",
    "                e_wins += 1\n",
    "    return e_wins / N\n",
    "\n",
    "def simulate_position_model(ranking, N):\n",
    "    e_wins = 0\n",
    "    for i in range(N):\n",
    "        for j in range(len(ranking)):\n",
    "            coin = random.uniform(0,1)\n",
    "            if coin < position_click_model.p[j]*ranking[j][\"rel\"] and ranking[j][\"source\"] == \"E\":\n",
    "                e_wins += 1\n",
    "    return e_wins / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rel': 0, 'source': 'P'}, {'rel': 1, 'source': 'E'}, {'rel': 0, 'source': 'P'}, {'rel': 1, 'source': 'E'}, {'rel': 0, 'source': 'P'}, {'rel': 1, 'source': 'E'}, {'rel': 2, 'source': 'P'}, {'rel': 1, 'source': 'E'}, {'rel': 2, 'source': 'P'}, {'rel': 2, 'source': 'E'}]\n",
      "0.686\n",
      "0.589\n"
     ]
    }
   ],
   "source": [
    "print(rankings[2000])\n",
    "print(simulate_random_model(rankings[2000], 1000))\n",
    "print(simulate_position_model(rankings[2000], 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Results and Analysis (30 points)\n",
    "Compare the results of the offline experiments (i.e. the values of the 𝛥measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n",
    "Use easy to read and comprehend visuals to demonstrate the results;\n",
    "Analyze the results on the basis of:\n",
    "- the evaluation measure used,\n",
    "- the interleaving method used,\n",
    "- the click model used.\n",
    "\n",
    "Report and ground your conclusions.\n",
    "\n",
    "(Note 8: This is the place where you need to demonstrate your deeper understanding of what you have implemented so far; hence the large number of points assigned. Make sure you clearly do that so that the examiner of your work can grade it accordingly.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra info\n",
    "Yandex Click Log File:\n",
    "The dataset includes user sessions extracted from Yandex logs, with queries, URL rankings and clicks. To allay privacy concerns the user data is fully anonymized. So, only meaningless numeric IDs of queries, sessions, and URLs are released. The queries are grouped only by sessions and no user IDs are provided. The dataset consists of several parts. Logs represent a set of rows, where each row represents one of the possible user actions: query or click.\n",
    "In the case of a Query:\n",
    "SessionID TimePassed TypeOfAction QueryID RegionID ListOfURLs\n",
    "\n",
    "\n",
    "In the case of a Click:\n",
    "SessionID TimePassed TypeOfAction URLID\n",
    "\n",
    "\n",
    "SessionID - the unique identifier of the user session.\n",
    "TimePassed - the time elapsed since the beginning of the current session in standard time units.\n",
    "TypeOfAction - type of user action. This may be either a query (Q), or a click (C).\n",
    "QueryID - the unique identifier of the request.\n",
    "RegionID - the unique identifier of the country from which a given query. This identifier may take four values.\n",
    "URLID - the unique identifier of the document.\n",
    "ListOfURLs - the list of documents from left to right as they have been shown to users on the page extradition Yandex (top to bottom)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
